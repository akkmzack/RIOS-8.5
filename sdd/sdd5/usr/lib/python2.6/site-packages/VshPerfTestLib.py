from xml.dom.minidom import parse
from sys import exit
from os import popen
from os.path import exists
from re import compile as recompile
from ModelPerfResults import *
from PerfUtil import *

# global variable indicating where we should look for 
# performance tools, when we're trying to run perf tests
perf_tool_path='/opt/rbt/bin'

flexpy_cmd='/opt/hal/bin/flexpy -s all'

## set_perf_tool_path
# @param new_path new default directory to search for performance tools
#
# Allows you to override the default tool path for all perf tools
#
def set_perf_tool_path(new_path):
    global perf_tool_path
    perf_tool_path = new_path

## def_o_routine
# @param output the string to display
# a default output routine to standard out, we include this
# as particular usages from the CLI may want to specify the use
# of cli_printf instead of stdout. cli_printf will buffer output
# until the end of the operation though, which is not suitable for 
# all use cases
# 
def def_o_routine(output):
    print output

def find_highest_model_test(curr_highest,model_specs,test, result):
    ret_model = curr_highest

    ph = popen(flexpy_cmd)
    try:
        output = ph.read().strip()
    except:
        return None

    ret = ph.close()
    if ret != None:
        success = False
    else:
        success = True

    models = output.split('\n')

    for model in models:
        perf_exp = None
        perf_act = None

        try:
            expected_result = model_specs.get_model_results_by_test_model(test, model)
            if expected_result == None:
                continue
            for test_res_key in expected_result.get_keys():
                metric = expected_result.get_metric(test_res_key)
                if not metric.compare_value(result):
                    break
                else:
                    ret_model = model
        except:
            pass

        if ret_model == curr_highest:
            break

    return ret_model

## find_highest_model
# @param current model
# This function returns the maximum runnable model
# based on the currently running model
# There is an impicit ordering here based on the L,M, H suffixes
def find_highest_model(model):
    if not model[0].upper() == 'V':
        raise AssertionError('Invalid model argument')
    model_ret = model
    # If the last digit is L, M or H return the H model
    # i.e. if it has the same segstore size
    if model[-1] in ['L', 'M', 'H']:
        model_num = model[1:-1]
    # 1050 is a special case so return 1050M for L/M
    # and 1050H for H
    if model_num == '1050':
        if not model[-1] == 'H':
            model_ret = model[0:-1] + 'M'
        else:
            model_ret = model[0:-1] + 'H'
    else:
        model_ret = model[0:-1] + 'H'

    return model_ret
       
## run_test_cmd
# @param utility name
# @param cmdline the given command line to run with the utility
#
def run_test_cmd(utility, cmdline):
    success=False
    full_util_path = '%s/%s' % (perf_tool_path, utility)
    full_cmdline = '%s/%s' % (perf_tool_path, cmdline)

    if not exists(full_util_path):
        return (success, 'Performance tool %s not found' % full_util_path)

    ph = popen(full_cmdline)
    try:
        output = ph.read().strip()
    except:
        success = False
        output = 'Exception while running test'

    ret = ph.close()
    if ret != None:
        success = False
    elif output == '':
        success = False
        output = 'No output returned from test'
    else:
        success = True

    return (success, output)


## SimpleTest class
# The simple test represents a particular test instance such as
# diskperf random read, etc
class SimpleTest:
    ## Initializer
    # @param xml xml node corresponding to a simple_test xml entry
    #
    def __init__(self, xml):
        self._name = ''
        self._desc = ''
        self._duration = 0
        self._utility = None

        if xml == None:
            raise AssertionError('Invalid xml argument')

        self.__populate_from_xml(xml)

    ## __str__
    # Return a string representation of the test object
    def __str__(self):
        result = ''
        result += '%s/name=%s\n' % (self._name, self._name)
        result += '%s/desc=%s\n' % (self._name, self._desc)
        result += '%s/duration=%d\n' % (self._name, self._duration)
        if self._utility == None:
            result += '%s/utility=None\n' % self._name
        else:
            result += '%s/utility=%s\n' % (self._name, 
                                           self._utility.get_cmdline_raw())
        return result

    ## get_name
    # Returns the name of this particular test
    def get_name(self):
        return self._name

    ## get_duration
    # Returns the duration associated with this test in string form
    def get_duration(self):
        return self._duration
        
    ## __populate_from_xml
    # @param xml simple_test xml node
    # Read the values in from XML
    def __populate_from_xml(self, xml):
        self._name = xml.getAttribute('name')
        self._desc = xml.getAttribute('desc')
        try:
            self._duration = int(xml.getAttribute('duration'))
        except ValueError:
            raise AssertionError('Invalid duration tag for %s' % self._name)

        utility = xml.getElementsByTagName('utility')[0]
        if utility != None:
            self._utility = PerfTestUtility(utility)
        else:
            raise AssertionError('Test config %s is missing the utility node' %
                                 self._name)

    ## run_test
    # @param model_data ModelData class containing model 
    #                   specific parameters that can be used for particular
    #                   tests
    # @param duration the duration of this particular test
    def run_test(self, 
                 model_data, 
                 duration=None):

        if duration == None:
            duration = self._duration

        cmdline = self._utility.get_cmdline('%d' % duration,
                               model_data.get_value(ModelData.model_disk_device),
                               model_data.get_value(ModelData.model_disk_size))
        if self._utility != None:
            (success, output) = run_test_cmd(self._utility.get_name(), cmdline)
        else:
            raise AssertionError('No utility associated with test %s' % self._name)

        if success:
            self._result = (success, self._utility.get_test_result(output))
        else:
            self._result = (False, None)

    ## is_destructive
    # indicates whether this test will write data to the system
    # requiring cleanup (e.g a disk write test)
    def is_destructive(self):
        return self._utility.is_destructive()

    ## get_result 
    # Return the result object associated with this test, the result
    # is a 2-tuple of (bool, result object) where bool is the status
    # of whether we had an error running the test and result is the 
    # Result object if the bool is true
    def get_result(self):
        return self._result
        

## SimpleTestMap
# A dictionary wrapping all the simple tests defined
# Simple test objects can be referenced by name
class SimpleTestMap:
    ## Initializer
    # @param xml the root perf_tests node
    def __init__(self, xml):
        self._simple_test_list = {}

        if xml == None:
            raise AssertionError('Invalid node argument')
        
        self.__populate_from_xml(xml)

    ## __str__
    # Return a string representation of the object
    def __str__(self):
        result = ''
        for stest_val in self._simple_test_list.values():
            result += str(stest_val)
        return result

    ## __populate_from_xml
    # @param xml the perf_tests root node of the xml config
    # Reads all simple tests in from the  xml file
    #
    def __populate_from_xml(self, xml):
        stest_cfg_entries = xml.getElementsByTagName('simple_test')
        for stest_cfg in stest_cfg_entries:
            stest = SimpleTest(stest_cfg)
            self._simple_test_list[stest.get_name()] = stest

    ## get_test_by_name
    # @param name the name of the indicated test
    # Returns a SimpleTest object referenced by name
    def get_test_by_name(self, name):
        if self._simple_test_list.has_key(name):
            return self._simple_test_list[name]
        else:
            return None
        
## PerfTest
# A perf test object is defined by a XML node defined in the config file
# that is an aggregation of any number of simple tests to be collected
# under the umbrella of the PerfTest
#
class PerfTest:
    ## Initializer
    # @param xml perf_test xml node
    def __init__(self, 
                 xml, 
                 simple_test_map):
        self._name = ''
        self._desc = ''
        self._hidden = ''
        self._sub_test_list = []
        self._stm = simple_test_map
        
        if xml == None:
            raise AssertionError('Invalid node argument')

        self.__populate_from_xml(xml)

    ## __str__
    # Returns a textual representation of the PerfTest object
    def __str__(self):
        result = ''
        result += '%s/name=%s\n' % (self._name, self._name)
        result += '%s/desc=%s\n' % (self._name, self._desc)
        result += '%s/hidden=%s\n' % (self._name, self._hidden)
        for stest in self._sub_test_list:
            for line in str(stest).strip().split('\n'):
                result += '%s/subtest/%s\n' % (self._name, line)
        return result
        
    ## __populate_from_xml
    # @param xml perf_test xml node
    # Reads the xml data and populates the object from xml
    def __populate_from_xml(self, xml):
        self._name = xml.getAttribute('name')
        self._desc = xml.getAttribute('desc')
        self._hidden = xml.getAttribute('hidden')
        for sub_test in xml.getElementsByTagName('sub_test'):
            st_name = sub_test.getAttribute('name')

            stest = self._stm.get_test_by_name(st_name)
            if stest == None:
                raise AssertionError('Perf test references invalid test %s' % (
                                     st_name))

            self._sub_test_list.append(stest)

    ## get_desc
    # Return the user visible description associated with this test
    def get_desc(self):
        return self._desc

    def get_hidden(self):
        return self._hidden

    ## is_destructive
    # Indicates whether this perf test will write any data to the system
    # requiring test cleanup (e.g writing to the segstore)
    def is_destructive(self):
        for test in self._sub_test_list:
            if test.is_destructive():
                return True
        return False
            
    ## run_test
    # @param model_data the Model Data class for use with this test 
    # @param duration a duration override if we want to run this
    #                 test longer than our default spec currently 
    #                 indicates
    # Output from here should use print, as we want
    # realtime status updates in the CLI
    # 
    # For Perf Tests with multiple simple tests, the user 
    # specified duration is to be the requested run time 
    # divided by the number of tests
    #
    def run_test(self, model_data, duration=None):
        print 'Running Performance Test: %s' % self._name
        if self.is_destructive():
            print '  Performance Test: DESTRUCTIVE' 
        else:
            print '  Performance Test: NON-DESTRUCTIVE'

        num_tests = len(self._sub_test_list)
        if duration != None:
            duration = int(duration)/num_tests
            print '  Sub Test Maximum Duration: %d' % duration

        for stest in self._sub_test_list:
            if duration == None:
                duration = stest.get_duration()

            print 'Running test: %s max duration %s seconds' % (stest.get_name(),
                                                            duration)
                                                           
            stest.run_test(model_data, duration)

    ## get_name
    # Returns the name of the PerfTest object
    def get_name(self):
        return self._name

    def __display_result_line(self, test_name, metric_name, min, 
                              exp, act, status,
                              output_routine):
        output_routine('  %10.10s %10.10s %10.10s %10.10s %10.10s %10.10s\n' % (
                       test_name,
                       metric_name,
                       min,
                       exp,
                       act,
                       status))


    ## display_results
    # @param model_specs specific model specifications for comparison
    #
    # Displays a user formatted summary of the performance tests
    # If model_specs is not specified, only the raw data is 
    # displayed, not the comparison
    #
    def display_results(self, 
                        model_specs=None, 
                        model_data=None,
                        output_routine=None):

        # if we are not given an output routine, use the default one
        if output_routine == None:
            output_routine = def_o_routine
        
        status_list = []
        highest_model = None

        for stest in self._sub_test_list:
            result = stest.get_result()
            
            if model_specs == None:
                if result[0] != True:
                    output_routine('%s: %s' % (stest.get_name(), 'Error'))
                else:
                    for test_res_key in result[1].get_keys():                    
                        output_routine('%s:%s: %s' % (stest.get_name(), 
                                             test_res_key,
                                             result[1].get_value(test_res_key)))
            else:
                if model_data == None:
                    raise AssertionError('No model data specified')

                # we have model specs, so compare against required values
                # for this model, if we do not have the model, raise 
                # an error
                model = model_data.get_value(ModelData.model_model)
                if model == None:
                    raise AssertionError('No model specified in model data')

                test_name = stest.get_name()

                if result[0] != True:
                    #output_routine('%s: %s %s' % (stest.get_name(), 
                    #               'Error', 'Error'))
                    self.__display_result_line(stest.get_name(),
                                               "", "", "", "", "TESTERROR",
                                               output_routine)
                else:
                    perf_exp = None
                    perf_act = None

                    expected_result = model_specs.get_model_results_by_test_model(test_name, model)
                    if expected_result == None:
                        self.__display_result_line(stest.get_name(),
                                                   "", "", "", "", "NORESULT",
                                                   output_routine)
                                                   
                    else:
                        # for test verification, we map each of the expected result keys
                        # to an output from the indicated specific TestResult object
                        # the rest result object can be any TestResult() or child of    
                        # the TestResult class 
                        for test_res_key in expected_result.get_keys():
                            metric = expected_result.get_metric(test_res_key)
                            perf_exp = metric.get_value(ModelPerfMetrics.expected_val_key)
                            perf_min = metric.get_value(ModelPerfMetrics.min_val_key)

                            perf_act = result[1].get_value(test_res_key)

                            try:
                                # the test metric identifies what range of
                                # value we should expect from a test
                                if not metric.compare_value(perf_act):
                                    status = 'FAIL'
                                else:
                                    status = 'PASS'
                            except ValueError:
                                status = 'Error'
                            except TypeError:
                                status = 'Error'
                            # add the status into the global status list  
                            status_list.append(status)
                            self.__display_result_line(
                                           stest.get_name(),
                                           test_res_key,
                                           perf_min,
                                           perf_exp,
                                           perf_act,
                                           status,
                                           output_routine)

                            highest_model = find_highest_model_test(
                                highest_model, model_specs, stest.get_name(),
                                perf_act)

        # Print the maximum capable model if all tests passed
        if 'FAIL' in status_list or 'Error' in status_list:
            output_routine('\nYour system is incapable of supporting the current model %s\n' % model)
        else: 
            output_routine('\nBased on this test your storage is capable of supporting up to model %s\n' % highest_model)

## PerfTestMap
# The perf test map is a dictionary of all defined perf tests
#
class PerfTestMap:
    ## Initializer
    # @param xml the root xml node for perf_tests 
    # @param simple_test_map the SimpleTestMap object defining all
    #        individual tests in the system
    #  
    def __init__(self, xml, simple_test_map):
        self._perf_test_map = {}
        self._simple_test_map = simple_test_map

        if xml == None:
            raise AssertionError('Invalid node argument')

        self.__populate_from_xml(xml)

    ## __str__
    # Returns a textual representation of all PerfTests
    def __str__(self):
        result = ''
        for ptest_val in self._perf_test_map.values():
            result += str(ptest_val)
        return result

    ## __populate_from_xml
    # @param xml the root xml perf_tests node
    # Reads the perf_test(s) in from XML and populates the map
    def __populate_from_xml(self, xml):
        ptest_cfg_entries = xml.getElementsByTagName('perf_test')
        for ptest_cfg in ptest_cfg_entries:
            ptest = PerfTest(ptest_cfg, self._simple_test_map)
            self._perf_test_map[ptest.get_name()] = ptest

    ## get_test_by_name
    # @param name the name of the PerfTest
    # Returns the PerfTest object
    def get_test_by_name(self, name):
        if self._perf_test_map.has_key(name):
            return self._perf_test_map[name]
        else:
            return None

    ## run_test_by_name
    # @param name the test name 
    # @param model_data ModelData information for use with this test
    # @param duration the time of the test
    # 
    # Runs a particular test by name with appropriate test 
    # parameters 
    #
    def run_test_by_name(self, name, model_data, duration=None):
        if model_data == None:
            print 'Invalid model data'
            return
        ptest = self.get_test_by_name(name)
        if ptest == None:
            print 'Invalid test name: %s' % name
        else:
            ptest.run_test(model_data, duration)

    def get_test_names(self):
        test_names = []
        for (test_name,value) in self._perf_test_map.items():
            if value.get_hidden() != 'true':
                test_names.append(test_name)

        return test_names

class ModelData:
    model_model='model'
    model_disk_device='diskdev'
    model_disk_size='disksize'

    def __init__(self, model=None, disk_device=None, disk_size=None):
        self._model_param_map = {}
        if disk_device != None:
            self._model_param_map[self.model_disk_device] = disk_device

        if model != None:
            self._model_param_map[self.model_model] = model

        if disk_size != None:
            self._model_param_map[self.model_disk_size] = disk_size

    def __str__(self):
        result = ''
        for key in self._model_param_map.keys():
            result += '%s=%s\n' % (key, self._model_param_map[key])
        return result

    def get_value(self, key): 
        if self._model_param_map.has_key(key):
            return self._model_param_map[key]
        else:
            return None

    def add_value(self, key, value):
        self._model_param_map[key] = value

## SystemTest 
# The system test object is used to set up the performance tests based on actual model
# parameters and running system locations
#
class SystemTest:
    xml_file_locations = '/opt/hal/lib/perf'
    test_spec_file     = 'perf_test_desc.xml'
    results_file       = 'perf_test_results.xml'
    clean_path         = '/var/opt/rbt/.clean'

    ## Initializer
    # @param spec_file user specified non-default test specs
    # @param result_file user specified non-default results file
    def __init__(self, spec_file=None, result_file=None):
        # use default values for the xml files
        # if we have not been told otherwise
        if spec_file == None:
            spec_file = '%s/%s' % (self.xml_file_locations, self.test_spec_file)

        if result_file == None:
            result_file = '%s/%s' % (self.xml_file_locations, self.results_file)

        spec_xml = parse(spec_file)
        self.__stm = SimpleTestMap(spec_xml)
        self.__ptm = PerfTestMap(spec_xml, self.__stm)
        # configure our default output to be print
        # which can be overridden by cli_printf for
        # use from CLI calls
        self.set_output_handler()

        self.__model_results = PerfTestResultMap(result_file)

    ## __default_test_output_routine
    # @param output
    # By default we'll output to stdout
    def __default_test_output_routine(self, output):
        print '%s' % output

    ## set_output_handler
    # @param output_routine user specified output routine
    # This allows a user to override the default output routine
    # with a routine such as cli_printf.
    def set_output_handler(self, output_routine=None):
        if output_routine == None:
            self.__output_routine = self.__default_test_output_routine
        else:
            self.__output_routine = output_routine
        
    ## get_perf_tests
    # Return a PerfTestMap object
    def get_perf_tests(self):
        return self.__ptm

    ## get_test_names
    # Return a list of PerfTest names
    def get_test_names(self):
        return self.__ptm.get_test_names()

    ## display_test_info
    # Displays formatted test names and descriptions
    def display_test_info(self):
        fmt = "  %-20.20s   %-50.50s\n"

        self.__output_routine (fmt % ('Test Name', 'Description'))
        for entry in self.__ptm.get_test_names():
            pt = self.__ptm.get_test_by_name(entry)
            name = pt.get_name()
            desc = pt.get_desc()
            
            self.__output_routine(fmt % (name, desc))

    ## has_test_by_name
    # @param test_name PerfTest name to use for lookup
    # Returns whether or not a test exists by name
    def has_test_by_name(self, test_name):
        return self.__ptm.get_test_by_name(test_name) != None

    ## mark_datastore_clean
    # For destructive write tests, we need to write out a file 
    # that tells sport to start clean.
    def mark_datastore_clean(self):
        if not exists(self.clean_path):
            try:
                fh = open(self.clean_path, 'w')
                fh.close()
            except (IOError, OSError):
                self.__output_routine('ERROR: Unable to touch store clean file, ' \
                                      'Datastore is corrupt, please restart clean')

    ## run_test_by_name
    # @param test_name Name of the Perf Test to run
    # @param model_data ModelData object that needs to contain
    #                   a model, diskdev, and disksize key/values
    # @param duration Optional duration override for the test.
    #
    # if a duration is specified it is considered to be the maximum duration
    # for all tests in the PerfTest.  Some perf tests may not require the maximum
    # amount of time, but the basic logic is to divide up the duration specified
    # into time slices allocated for each test.  A given test should not exceed its
    # duration, but may run in less time than its duration (for 
    # example one of the random read diskperf tests is very short and does not
    # take a command line duration parameter)
    def run_test_by_name(self, test_name, model_data, duration=None):
        pt = self.__ptm.get_test_by_name(test_name)

        destructive = False
        try:
            destructive = pt.is_destructive()
            if destructive:
                continue_test = raw_input('Test will destroy your segment store. Continue? (Y/N)')
                if continue_test.strip().lower() == 'y':
                    pt.run_test(model_data, duration)
                else:
                    self.__output_routine('Abort')
                    return
            else:
                pt.run_test(model_data, duration)
        except AssertionError, what:
            self.__output_routine(what)
            # if this was a destructive test we still need
            # to signal a clean of the datastore.
            if destructive:
                self.mark_datastore_clean()

            return
        
        if destructive:
            self.mark_datastore_clean()

        self.__output_routine('\nTest Result Summary:\n')
        self.__output_routine('  %10.10s %10.10s %10.10s %10.10s %10.10s %10.10s\n' % (
                              'Test Name', 
                              'Metric', 
                              'Min', 
                              'Expected', 
                              'Measured',
                              'Status'))
        pt.display_results(model_specs = self.__model_results,
                           model_data  = model_data,
                           output_routine = self.__output_routine)
