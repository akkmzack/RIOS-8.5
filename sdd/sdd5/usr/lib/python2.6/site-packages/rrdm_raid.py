#!/usr/bin/env python

from sys import exit, argv
from getopt import getopt, GetoptError
from re import compile as recompile
from os import remove, popen, stat, path, rename, listdir
from time import sleep
from rrdm_util import rrdm_error,HwtoolDriveList,run_shell_cmd,get_scsi_sysfs_param,get_sysfs_param, rlog_debug
from rrdm_util import *
from rrdm_super import RvbdSuperBlock
from rrdm_disk import HardDisk, PartitionTable, DiskArray, Partition
from rrdm_config import SysfsXmlConfig
from rrdm_sysconfig import SystemConfig

try:
    from Logging import *
except ImportError:
    pass


################################################################################
# Raid Partition
# 
# A disk partition extended to contain raid information
#
################################################################################
class RaidPartition(Partition):
    def __init__(self):
        Partition.__init__(self)
        self.raid_array=None
        self.raid_port = ''
        self.raid_status='online'
	self.__raid_super = None

    ###########################################################################
    # raid_super
    #
    # Retrieve the raid superblock class for this partition...
    #
    ###########################################################################
    def raid_super(self):
	if self.__raid_super != None:
	    return self.__raid_super

	self.__raid_super = RaidSuper('/dev/%s' % self.dev_name)
	return self.__raid_super

    ###########################################################################
    # has_raid_sb
    #
    # True if this raid partition has a valid raid SB
    # False otherwise.
    #
    # FIXME: this should be removed when everything is converted to
    #        use raid_super().xxx()
    #
    ###########################################################################
    def has_raid_sb(self):
	rdev_name = '/dev/%s' % self.dev_name
	try:
	    SB=read_md_sb(rdev_name)
	except rrdm_error:
	    return False
	
	return True

    ###########################################################################
    # get_raid_sb_uid
    #
    # Read the MD UID from the given partition.  You only want to do this
    # when you need this info, as accessing the SB often is slow, given
    # the number of disks and raid arrays in a typical system.
    #
    # FIXME: This could be: raid_super().uuid(), which is cached so faster,
    #        but w/ slightly different semantics 
    #
    ###########################################################################
    def get_raid_sb_uid(self):
	if not self.is_missing():
	    try:
		rdev_name = '/dev/%s' % self.dev_name
		dev_sb_output = read_brief_md_sb(rdev_name)
		uid = get_uid_from_brief_sb(dev_sb_output)
		return uid
	    except (rrdm_error, OSError, IOError):
		return 'none'

	return 'none'

    ###########################################################################
    # check_consistency
    # 
    # Makes sure that the Disk and the RAID agree about what disk/port this
    # array should be on.
    #
    # Return True for in sync or missing,
    #        False otherwise
    ###########################################################################
    def check_consistency(self):
	if not self.is_missing():
	    rlog_debug ('Checking consistancy on %s' % self.dev_name)
	    rdev_name = '/dev/%s' % self.dev_name

            if exists (rdev_name):
	        dev_sb_output = read_brief_md_sb(rdev_name)
	        rdev    = get_rdev_from_brief_sb(dev_sb_output)
	        sb_rdev = self.hd.superblock.get_raid_port() 

	        rlog_debug ('%s consistency [%s:%s]' % (self.get_devname(),
			    rdev, sb_rdev))
	        if rdev != sb_rdev:
		    return False

	return True

    def add(self):
        if not self.is_ok():
            array_name = self.raid_array.get_devname()
            disk_dev   = 'disk%sp%s' % (self.hd.portnum, self.part_id)
            raid_drive = self.hd.portnum

            mdadm_cmd='/sbin/mdadm --zero-superblock /dev/%s' % (disk_dev)
            rlog_debug ('executing command %s' % mdadm_cmd)

            if run_shell_cmd(mdadm_cmd, False) != 0:
                print 'Failed to zero superblock on [%s]' % (disk_dev)
            else:
                print 'Successfully wiped out the superblock on [%s]' % (disk_dev)

            mdadm_cmd='/sbin/mdadm --add -d %s /dev/%s /dev/%s' % (raid_drive, array_name, disk_dev)
            rlog_debug ('executing command %s' % mdadm_cmd)

            if run_shell_cmd(mdadm_cmd, False) != 0:
                print 'Failed to add drive [disk%s] to raid [%s]' % (self.hd.portnum, self.dev_name)      
            else:
                print '%s successfully added to array %s' % (self.hd.get_devname(),
                        self.dev_name)
        else:
            print '%s is already online in array %s' % (self.hd.get_devname(), self.dev_name)


    def fail(self):
        # once you've failed the disk, it disappears from the sysfs entry,
        # you can only fail a drive once, also b/c of that read the dev name first.
        #
        # failing is a 2 stage process of setting the drive to faulty and removing it
        # from the array.
        #
        array_name      = self.raid_array.get_devname()

        # XXX currently assumes that the disk in port X is raid X
        #
        if self.raid_port == 'unknown':
            # if this drive isnt in the system assume its on the hard drive.
            rlog_debug ('drive has been removed using drive-raid map')
	    sysconfig = SystemConfig()
	    if sysconfig.is_config_valid():
		portnum = sysconfig.get_disk_rport(self.hd.portnum)
	    else:
		# if we don't know which raid port to fail, don't just continue on.
		# skip out and log a msg.
		#
		rlog_notice ('Unable to determie rport when failing disk [%s]' %
			     portnum)
		return
        else:
            portnum = self.raid_port

        state_cmd   = "faulty"
        remove_cmd  = "remove"

	md_devname_path = '/sys/block/%s/md/rd%s/device' % (array_name, portnum)

        try:
            md_dev_name = get_sysfs_param (md_devname_path)
        except IOError:
            raise rrdm_error ('unable to read raid device : %s' % md_devname_path)

	# use the device name indicated by RAID, since if the drive is missing,
	# md might still have a reference to the device, but we don't have a scsi device
	# to use to figure out what the name of the device that used ot be in the array
	# is
        md_state_path  = '/sys/block/%s/md/dev-%s/state' % (array_name, md_dev_name)

        rlog_notice ('Failing array [%s] device [%s:%s]' % (array_name,
                      portnum, md_dev_name))
	retries = 0

	while retries < 3:
	    try:
		if exists (md_state_path):
		    sys_file = open (md_state_path, "w")
		    try:
			sys_file.write(state_cmd)
		    finally:
			sys_file.close()

		    sleep (0.5)

		    sys_file = open (md_state_path, "w")
		    try:
			sys_file.write(remove_cmd)
		    finally:
			sys_file.close()

		    # if we succeed, give a grace period to allow for the request 
		    # to complete.
		    sleep (0.5)

		# bail out its failed already or we succeeded
		# make sure drive is really gone, and if its not.. retry
		if not exists (md_state_path):
		    break
		    
	    except IOError:
		retries += 1

	if exists (md_state_path):
	    rlog_debug('Unable to fail %s on %s with cmd [%s:%s]' % (
		       self.raid_port, array_name, md_state_path,remove_cmd))
	    


    def make_missing_partition(self, array, port):
        self.raid_array = array
        self.raid_port = port
        self.raid_status = 'missing'

    def copy_partition(self, part):
        self.part_id = part.part_id 
        self.dev_name = part.dev_name
        self.size = part.size 
        self.type = part.type 
        self.hd = part.hd   
        
        if self.hd != None:
            self.serialnum = self.hd.serialnum

    def make_partition(self, part_id, hd, raid_array, rport='unknown', rstatus='missing'):
	Partition.make_partition(self, part_id, "0", 'raid', hd)
	self.raid_array	    = raid_array
	self.raid_port	    = rport
	self.raid_status    = rstatus
	self.serialnum	    = hd.serialnum
	
    def fill_from_system_info(self):
        if (self.hd.status == 'missing'):
            self.raid_port = -1
            self.raid_status = 'missing'

        path = '/sys/block/%s/md/dev-%s/slot' % (self.raid_array.dev_name, self.dev_name)
        try:
            self.raid_port = int (get_sysfs_param (path) , 10)
        except Exception:
            self.raid_port = -1
            self.raid_status = 'missing'

    # Check to see if this Raid Partition is on the drive passed in.
    #
    def is_on_drive(self, hd):
        return self.hd.portnum == hd.portnum

    def is_ok (self):
        return self.raid_status == 'online'

    def is_missing (self):
	return self.raid_status == 'missing'

    def is_rebuilding(self):
        if self.raid_status == 'rebuilding':
            return True
        else:
            return False

    def get_status (self, xml = False):
        if xml == True:
            print '<raid-drive port=\"%s\" status=\"%s\"/>' % (self.raid_port, self.raid_status)
        else:
            print '\t%s\t%s' % (self.raid_port, self.raid_status)

    def get_detail_status(self, xml = False):
        # handle the hd port association sep. since the hd might not exist.
        if self.hd == None:
            port_num = 'unknown'
        else:
            port_num = self.hd.portnum

        if xml == True:
            print '<raid-drive port=\"%s\" drive=\"%s\" part=\"%s\" status=\"%s\"/>' % (self.raid_port, port_num, self.part_id, self.raid_status)
        else:
            print '\t%s\t%s\t%s\t%s' % (self.raid_port, port_num, self.part_id, self.raid_status)


################################################################################
# RaidArray 
#
#
################################################################################
class RaidArray:
    def __init__(self):
        self.part_list=[]
        self.type=''

    # state is running or stopped, which allows us to determine 
    # how to tell if an array is broken or not, if it is stopped
    # we need to be able to determine if it has failed, and thats why it stopped.
    state           = 'stopped'

    # status tells us whether the structure of the array is ok or not
    # 
    num_drives      = 0
    found_devices   = 0
    status          = ''
    array_id        = ''
    chunk           = '64'
    level           = 'ThisRaidLevelShouldAlwaysBeSuperceded'
    bitmap          = None
    part_list       = []
    dev_name        = ''
    fstype          = ''
    name	    = ''
    uuid	    = ''
    layout          = ''

    # Rebuild State Info
    # 
    sync_complete_kb	= 0
    sync_total_kb	= 0
    mdadm_metatdata     = ''

    def get_devname(self):
        return self.dev_name

    def is_stopped(self):
	return self.status not in ['online', 'degraded', 'rebuilding']

    def is_online(self):
        return self.status == 'online'

    def is_degraded(self):
        if self.status in  ['degraded', 'rebuilding']:
            return True
        else:
            return False

    def is_rebuilding(self):
        return self.status == 'rebuilding'

    ###########################################################################
    # display_configuration
    #
    # display user visible RAID configuration.
    #
    # TODO - make a level module so that this is cleaner once we add multiple 
    # RAID types to manage.
    ###########################################################################
    def display_configuration(self, order = None):
	print '------------------------------------------------------'
	print '-- %s' % self.name
	print '------------------------------------------------------'

	if self.level == 'raid10':
            mirror_dict = {}
            if order != None:
                # arrange in mirror groups by raid port.
                cnt = 0
                # Use the ordering defined in specs instead of the default ascending
                # ordering for disks
                for port in order:
                    mirror_num = cnt/2
	            for rpart in self.part_list:
		        if int(rpart.hd.portnum) == port:
		            if mirror_dict.has_key(mirror_num):
		                entry = mirror_dict[mirror_num]
		                entry.append(rpart)
		            else:
		                entry = []
		                entry.append(rpart)
		                mirror_dict[mirror_num] = entry
                    cnt += 1

                for mirror in range(0, self.num_drives):
                    if mirror_dict.has_key(mirror):
                        entry = mirror_dict[mirror]
                        lines = []

                        mirror_status = 'online'
                        found = 0
                        if len(entry) == 0:
                            print 'MIRROR-%d\tfailed' % (mirror)
                            continue

                        for item in entry:
                            # should really sort these..
                            lines.append('RAID-%s\t\t%s\t%s' % (item.hd.portnum, item.status, item.size))
                            found = found + 1
                            if not item.is_ok():
                                mirror_status = 'degraded'

                        if found < 2:
                            mirror_status = 'degraded'

                        print 'MIRROR-%d\t%s' % (mirror, mirror_status)
                        for line in lines:
                            print '%s' % line

            else:
                # arrange in mirror groups by raid port. Default ordering is ascending
	        for rpart in self.part_list:
		    mirror_num = rpart.raid_port / 2
		    if mirror_dict.has_key(mirror_num):
		        entry = mirror_dict[mirror_num]
		        entry.append(rpart)
		    else:
		        entry = []
		        entry.append(rpart)
		        mirror_dict[mirror_num] = entry

                for mirror in range(0, self.num_drives):
	            if mirror_dict.has_key(mirror):
		        entry = mirror_dict[mirror]
		        lines = []
    
		        mirror_status = 'online'
                        found = 0
		        if len(entry) == 0:
		            print 'MIRROR-%d\tfailed' % (mirror)
		            continue
			
		        for item in entry:
		            # should really sort these..
		            lines.append('RAID-%s\t\t%s\t%s' % (item.raid_port, item.status, item.size))
		            found = found + 1
		            if not item.is_ok():	
		                mirror_status = 'degraded'
		    
		        if found < 2:
		            mirror_status = 'degraded'
		    
		        print 'MIRROR-%d\t%s' % (mirror, mirror_status)
		        for line in lines:
		            print '%s' % line
        elif self.level == 'raid5':
            print 'Deciding what to display for Raid-5'
        elif self.level == 'raid6':
            print 'Deciding what to display for Raid-6'
	else:
	    print '%s has unknown raid configuration : %s' % (self.get_devname(), 
		   self.level)

    def check_consistency(self):
	in_sync = True
	for part in self.part_list:
	    # only check actively in sync drives since MD will use a
	    # spare drive number during the rebuild sequence, which can trick this into
	    # thinking its out of sync
	    if part.check_consistency() and not part.is_ok():
		in_sync = False
		rlog_notice ('On array [%s] disk [%s] device [%s] is out of sync' % (
			     self.get_devname(),
			     part.hd.get_devname(),	
			     part.get_devname()))

	if not in_sync:
	    rlog_notice ('System may be vulnerable to single disk failure')
	    raise rrdm_error ('Array %s is not properly spread across system drives' % (
			      self.get_devname()))

	return in_sync
	    
    ###########################################################################
    # purge_faulty_drives
    # 
    # There is a vulnerability in the raid stuff where if the sysfs entry
    # for rdX is not present, we can't figure out what the device name of the
    # drive is to remove it from the array, which prevents us from syncing the
    # state properly.
    # 
    # if we get a drive in the array where we don't have the rdX, but md still
    # detects it with the dev-sdYZ entry, then partitioning the drive will fail
    # since md still has claimed that disk.
    #
    # this method will go through the raid entities and remove any faulty drive,
    # so we can ensure that md won't claim any drives that are failed.
    #
    ###########################################################################
    def purge_faulty_drives(self):
	dev_entry_re = recompile ("^dev")

	# walk the dev-sdX entries in the sysfs for a raid device and remove any
	# that are faulty.
	md_rootdir  =	'/sys/block/%s/md/' % self.get_devname()
	try:
	    dir = listdir(md_rootdir)
	    for d in dir:
		if dev_entry_re.match (d):
		    state_entry = '%s%s/state' % (md_rootdir, d) 
		    try:
			state	    = '%s' % get_sysfs_param(state_entry)
		    except (IOError, OSError):
			# ignore and continue
			continue
		
		    if state == "faulty":
			rlog_debug ('Cleaning up stale device [%s] reference in array [%s]' % (
				    d, self.get_devname()))
			# we found a disk that should have been removed but wasnt
			if not set_sysfs_param (state_entry, 'remove'):
			    rlog_notice ('Unable to remove faulty device [%s] from array [%s]' % (
					 self.get_devname(),
					 d))
	except (IOError, OSError):
	    # make sure we keep on going if we have a problem, we want to try to
	    # fix any inconsistancies found
	    pass
	    
    # drives that have failed can be re-added
    # but the raid array won't relinquish their resources unless 
    # they are removed from the array first.
    #
    def hot_add_drive(self, hd):
        # the logical drive of the hd is the raid drive
        # also we need to look up this disk in our raidcfg and see 
        # which partition is on which disk, a special note,
        # nothing prevents a disk having multiple partitions in
        # the same array
        rdev_list = self.__device_list.find_devices_on_disk(hd.portnum)
        for rdev in rdev_list:
            part_num = rdev.part_id
            # fill in the raid drive based on the logical device
            # associated with the device config
            raid_drive = rdev.get_logical_device()
            disk_dev = 'disk%sp%s' % (hd.portnum, part_num)
            if self.is_degraded():
                mdadm_cmd='/sbin/mdadm --zero-superblock /dev/%s' % (disk_dev)
                rlog_debug ('executing command %s' % mdadm_cmd)

                if run_shell_cmd(mdadm_cmd, False) != 0:
                    print 'Failed to zero superblock on [%s]' % (disk_dev)
                else:
                    print 'Successfully wiped out the superblock on [%s]' % (disk_dev)

                mdadm_cmd='/sbin/mdadm --add -d %s /dev/%s /dev/%s' % \
                    (raid_drive, self.dev_name, disk_dev)
                rlog_debug ('executing command %s' % mdadm_cmd)

                if run_shell_cmd(mdadm_cmd, False) != 0:
                    print 'Failed to add drive [disk%s] to raid [%s]' % \
                          (hd.portnum, self.dev_name)       
                else:
                    print '%s successfully added to array %s' % \
                          (hd.get_devname(),
                           self.dev_name)

            if self.is_online():
                print '%s is already online in array %s' % \
                    (hd.get_devname(), self.dev_name)

    # find the hd in the array and get its status within the array.
    def get_drive_raid_status(self, hd):
        if not self.is_disk_in_raid(hd.portnum):
            return None

        if not self.is_stopped():
            for part in self.part_list:
                if part.is_on_drive(hd):
                    if part.is_rebuilding():
                        return 'rebuilding'
                    elif not part.is_ok():
                        return 'failed'
                    else:
                        return 'online'

        # we didnt find a match, but should have.
        return 'failed'

    def find_dev_by_hd_portnum(self, port):
        for rpart in self.part_list:
            if rpart.hd.portnum == port:
                return rpart
        return None

    def find_dev_by_logical_devnum(self, port):
        for rpart in self.part_list:
            if rpart.raid_port == port:
                return rpart

        return None

    def find_dev_by_hd (self, hd):
        for rpart in self.part_list:
            if hd == rpart.hd:
                return rpart

        return None

    def find_dev_by_raid_id(self, id):
        for part in self.part_list:
            # if its a failed raid drive, raid doesnt tell us where it came from.
            id_str = '%d' % id

            if part.raid_port == id_str:
                return part

        raise rrdm_error ('No raid device %s' % part.part_id)

    def determine_array_status(self):
        # kinda weird but MD can leave an array device present with
        # array status clean so we need to check that too.
        try:
            stat ('/dev/%s' % self.dev_name)
        except OSError:
            return 'stopped'

        if self.level != 'linear':
            try:
                array_state = get_sysfs_param ('/sys/block/%s/md/array_state' % self.dev_name)
                sync_action = get_sysfs_param ('/sys/block/%s/md/sync_action' % self.dev_name)
                num_failed  = get_sysfs_param('/sys/block/%s/md/failed_disks' % self.dev_name)
                # check the number of failed disks and then see if we're syncing
                #
                if int (num_failed, 10) != 0:
                    if sync_action == 'idle':
                        return 'degraded'
                    else:
                        return 'rebuilding'
                else:
                    return 'online'
            except IOError:
                return 'stopped'
        else:
            # linear raid levels do not support sync/failed disks
            # we simply assume these arrays are online, as they
            # do not maintain raid drive element state
            # if the raid array device exists, it is online
            #
            return 'online'

    def is_disk_in_raid(self, drive):
        return self.__device_list.is_disk_in_devlist(drive)

    def collect_rebuild_info(self):
	try:
	    sync_state   = get_sysfs_param('/sys/block/%s/md/sync_completed' % self.dev_name)
	    values=sync_state.strip().split("/")
	    self.sync_completed_kb  = int(values[0])
	    self.sync_total_kb	    = int(values[1])
	    rlog_debug ("Rebuild info : [%d:%d]" % (self.sync_completed_kb, self.sync_total_kb))
	
	except (rrdm_error, IndexError, ValueError):
	    self.sync_total_kb = -1
	    self.sync_complete_kb = 0

    # Note:
    # using MD to fill in system information is a bit off since, it loses state between reboots,
    # and some entries are available when the array is degraded and some are not.
    # an alternate approach is to fill the array in based on the disk array,
    # and map each disk to an appropriate raid drive, then we don't need
    # to worry about differences between what MD tells us and what the system tells us
    # in the different cases if drive removal/ drive failure/ etc.
    #
    def fill_from_system_info(self,
                             device_list,
                             name,
                             dev_name,
                             fstype,
                             type,
                             layout,
                             level,
                             cfg_size_mb,
                             md_meta = None, # <sigh> this now has to be supplied, even for xx50s
                             zone = 'mgmt',  # to maintain compatibility with xx50 models
			     sysfscfg_list = []):
        self.dev_name   = dev_name
        self.name       = name
        self.fstype     = fstype
        self.type       = type
        self.layout     = layout
        if level == 'raid6b':
            self.level = 'raid6'
            self.bitmap = 'internal'
        else:
            self.level  = level
        self.zone = zone
        self.mdadm_metadata=md_meta
        self.cfg_size_mb = cfg_size_mb

        self.__device_list = device_list
	self.__sysfscfg_list = sysfscfg_list

        # currently we expect each raid to go across all drives
        # in the system 
        self.num_drives = self.__device_list.get_expected_drives()

        self.status = self.determine_array_status()
	if self.is_rebuilding():
	    self.collect_rebuild_info()

	if not self.is_stopped():
	    try:
		self.uuid = get_sysfs_param ('/sys/block/%s/md/uuid' % self.dev_name)
	    except rrdm_error:
		self.uuid = ''
	    
        rlog_debug ('raid status for [%s] is [%s]' % (self.dev_name, self.status))
        for diskpart in self.__device_list.get_devices():
            part_num = diskpart.part_id
            disk = diskpart.hd
            rlog_debug ('adding disk device for raid array [%s] part [%s]' % \
                        (self.dev_name, 
                         part_num))

            rpart = diskpart.get_devname()
            rdevice = '/dev/%s' % rpart

            # This is an assumption that should hold true even on old boxes,
            # the raid port should equal the logical port carried by the device in
            # the drive list.
            # originally we simply encoded the raid port in the rvbd SB as the drive
            # number.  This would be an issue if we supported moving around disks,
            # but as we don't support that today, we should be ok.
            # the problem with moving drives around would be that each drive physically
            # could now be a different rdev in a number of arrays, and the SB doesnt store
            # this well today
            # 
            rdev = diskpart.get_logical_device() 
	
#            try:
#		# if the disk has a valid riverbed SB, we can use the SB info to give us
#		# the raid port, otherwise we need to fall back to using mdadm to get the
#		# raid port.
#		#
#		if disk.has_valid_superblock():
#		    rdev = disk.superblock.get_raid_port()
#		    rlog_debug('Superblock indicates [%s] is [%s]' % (rpart, rdev))
#		elif not disk.is_failed():
#		    # fallback to mdadm's brief superblock output and get the raid port from there.
#		    rlog_debug ('Disk %s has no riverbed superblock, checking mdadm' % rpart)
#		    dev_sb_output = read_brief_md_sb(rdevice)
#		    # we expect a string rdev here.
#		    rdev = '%s' % get_rdev_from_brief_sb(dev_sb_output)
#                else:
#                    rdev = 'unknown'
#                    raise rrdm_error ("Disk %s doesnt have a riverbed superblock" % rpart)
#            except rrdm_error:
#
#                # we can't read the SB info for this disk and we know its not missing, so..
#                # fill it in failed and go to the next disk
#                newpart=RaidPartition()
#		# here we need to use a fallback from the config if the drive is missing,
#		# and we want movable drives.
#		newpart.make_partition(part_num, disk, self, disk.portnum)
#                newpart.device_name = '%s' % rpart
#
#                self.found_devices = self.found_devices + 1
#                self.part_list.append(newpart)
#                continue

            if rdev == 'unknown':
                continue

            rlog_debug ('disk [%s] is [%s] raid drive [%s]' % \
                        (rpart, self.dev_name, rdev))
            
            base_dev=hwtool_disk_map.find_devname_by_port(disk.portnum)
            base_devname='%s%s' % (base_dev, part_num)
            
            path='/sys/block/%s/md/dev-%s/state' % (self.dev_name, base_devname)
            try:
                disk_state=get_sysfs_param(path)
                disk_status = convert_md_status_to_rrdm(disk_state)
            except IOError:
                disk_status='failed'

            newpart=RaidPartition()
	    newpart.make_partition (part_num, disk, self, rdev, disk_status)
            newpart.device_name = '%s' % rpart

            self.part_list.append(newpart)
            self.found_devices = self.found_devices + 1
            continue

    def get_zone(self):
        return self.zone

    def get_status (self, xml = False):
        if xml == True:
            print '<raid-array name=\"%s\" status=\"%s\">' % (self.name, self.status)
        else:
            print '%s\t%s' % (self.name, self.status)

        if xml == True:
            print '</raid-array>'

    def get_detail_status(self, xml = False):
        if xml == True:
            print '<raid-array name=\"%s\" devname=\"%s\" status=\"%s\">' % (self.name, self.dev_name, self.status)
        else:
            print '%s\t%s\t%s' % (self.name, self.dev_name, self.status)

        for dev in self.part_list:
            dev.get_detail_status (xml)

        if xml == True:
            print '</raid-array>'

    ###########################################################################
    # __valid_device_list
    #
    # figure out the list of devices which belong to the array with
    # uuid.
    # 
    # This will return a list of devices for the given uuid
    #
    ###########################################################################
    def __valid_device_list(self, uuid):
	# we're only interested in devices which are alive...
	plist = filter(lambda dev: not dev.hd.is_missing(), self.part_list)

	# we want devices with a valid superblock
	plist = filter(lambda dev: dev.raid_super().magic() != '', plist)

	# select devices with the correct uuid
    	plist = filter(lambda dev: dev.raid_super().uuid() == uuid, plist)

	# find max events...
	if plist == []:
		return []
	maxe = max(map(lambda dev: dev.raid_super().events(), plist))

	# select only devices with events close enough to max...
	return filter(lambda dev: dev.raid_super().events() + 1 >= maxe, plist)


    ###########################################################################
    # __invalid_device_list
    #
    # figure out the list of devices which are in the slots of the
    # array but which do not belong to the array uuid.
    # 
    ###########################################################################
    def __invalid_device_list(self, uuid):
	# we're only interested in devices which are alive...
	plist = filter(lambda dev: not dev.hd.is_missing(), self.part_list)

	# get the list of valid devices...
	vlist = self.__valid_device_list(uuid)

	# an invalid device has no valid device entry...
	return filter(lambda dev: vlist.count(dev) == 0, plist)

    ###########################################################################
    # __form_mdadm_assemble_device_list
    #
    # figure out the list of devices we should be starting an array with.
    # the assemble option can't handle mixed array UID assemblies, or 
    # assembling raids where some of the devices don't have superblocks.
    # 
    # This will return a list of device strings for the given uuid
    #
    ###########################################################################
    def __form_mdadm_assemble_device_list(self, uuid):
	# get valid list of devices
	ulist = self.__valid_device_list(uuid)

        # If we have a superblock > 1.0 version then the parsing is different
        # the newer superblocks use role to define where the RAID port is
        # instead of using the big table used in 0.9 version
        if ulist[0].raid_super().version().startswith('1'):
   	    # select only devices which ere not spares...
	    ulist = filter(lambda dev: dev.raid_super().get_role() < self.num_drives, ulist)
        else:
            ulist = filter(lambda dev: dev.raid_super().lookup_disk('this').raid_device() < self.num_drives, ulist)

	# and return the list of device paths...
	return map(lambda dev: '/dev/%s ' % dev.dev_name, ulist)
    
    ###########################################################################
    # __spare_device_list
    #
    # figure out the list of devices which should be added to an array
    # immediately after starting the array.  These are the "spare" devices,
    # and if we add them at assemble time they will be assigned random
    # free slots in the array.
    # 
    # This will return a list of devices for the given uuid
    #
    ###########################################################################
    def __spare_device_list(self, uuid):
	# get valid list of devices
	ulist = self.__valid_device_list(uuid)

	# return spare devices
        if ulist[0].raid_super().version().startswith('1'):
	    return filter(lambda dev: dev.raid_super().get_role() >= self.num_drives, ulist)
        else:
            return filter(lambda dev: dev.raid_super().lookup_disk('this').raid_device() >= self.num_drives, ulist)

    ###########################################################################
    # __form_mdadm_create_device_list
    #
    # figure out the list of devices we should be creating an array with.
    # 
    # This will return a list of device strings for the given uuid
    #
    ###########################################################################
    def __form_mdadm_create_device_list(self):
	create_list = []

	# for recreation just put all the devices in the list.
	for dev in self.part_list:
	    if not dev.hd.is_missing():
		create_list.append('/dev/%s ' % (dev.device_name))

	return create_list

    #
    # create an mdadm command line from option and device lists
    #
    def __form_mdadm_cmd_line(self, opt_list, dev_list):
	cmd_list = ['mdadm ']
	cmd_list.extend(opt_list)
	cmd_list.extend(dev_list)
	return ''.join(cmd_list)

    #
    # options common to create and assemble
    #
    def __form_mdadm_common_opt_list(self):
	cmd_line = []
        cmd_line.append('/dev/%s ' % self.dev_name)
        cmd_line.append('-c %s ' % self.chunk)
        cmd_line.append('--force ');
	return cmd_line

    def __form_mdadm_create_opt_list(self):
        cmd = '--create --level=%s --raid-devices=%d ' % (self.level, self.num_drives)
	cmd_line = [ cmd ]
	if self.layout != '':
	    cmd_line.append('--layout %s ' % self.layout)
	if self.bitmap != None:
	    cmd_line.append('--bitmap=%s ' % self.bitmap)
	cmd_line.extend(self.__form_mdadm_common_opt_list())

        # We have to allow setting of the values for "--metadata="
        # if there is none set in the storage_config in specs.xml,
        # we'll set it to the legacy version of 0.9.
        # if we get the keyword 'default', we'll take whatever the
        # default is (no --metadata= to mdadm).
        # otherwise, take the version set in specs.xml

        md_meta=self.mdadm_metadata

        if md_meta == 'default':
            pass
        elif md_meta == None:
            cmd_line.append ('--metadata=0.90 ')
        else:
            cmd_line.append ( '--metadata=' + md_meta +' ')

        cmd_line.append('--run ');
	return cmd_line
	
    def __form_mdadm_assemble_opt_list(self):
	cmd_line = ['--assemble ']
	cmd_line.extend(self.__form_mdadm_common_opt_list())
	return cmd_line
	
    #
    # given a RaidPartition (dev), create an add line for it...
    #
    def __form_mdadm_add_opt_list(self, dev):
	cmd_line = ['--add ']
	cmd_line.append('-d %d ' % dev.raid_port)
	cmd_line.append('/dev/%s ' % self.dev_name)
	return cmd_line

    def create_raid_array(self, dry = False):
#       if self.status != 'stopped' and not dry:
#           raise rrdm_error ('RAID device %s is already running' % self.dev_name)

        if (self.found_devices == self.num_drives):
	    dev_list = self.__form_mdadm_create_device_list()
	    if len(dev_list) == 0:
		raise rrdm_error ('Insufficient raid disks to create array [%s]' % 
				  self.dev_name) 

	    opt_list = self.__form_mdadm_create_opt_list()
	    command = self.__form_mdadm_cmd_line(opt_list, dev_list)

            if dry:
                print 'running:', command
            else:
                try:
                    ret = run_shell_cmd(command)
                except rrdm_error:
                    try:
                        ret = run_shell_cmd('/mfg/'+command)
                    except rrdm_error:
                        raise rrdm_error('failed to start RAID with cmdline : %s' % command)
        else:
            raise rrdm_error ('Unable to create raid array with missing disks [%d/%d]' % (self.found_devices, self.num_drives))

        rlog_notice ('Created array [%s:%s]' % (self.name, self.dev_name))

    #
    # assemble the raid array, return true if started
    #
    def __assemble_raid_array(self, uuid, dry):
	dev_list = self.__form_mdadm_assemble_device_list(uuid)
	opt_list = self.__form_mdadm_assemble_opt_list()
        cmd_line = self.__form_mdadm_cmd_line(opt_list, dev_list)
	started_array = False
	if dry:
	    print 'Running:', cmd_line
	else:
	    try:
	        if len(dev_list) == 0:
		    raise rrdm_error ('Insufficient raid disks to start array [%s]' % self.dev_name) 
		rlog_notice ('Raid Assemble: [%s]' % cmd_line)
		run_shell_cmd(cmd_line)
		started_array = True
	    except rrdm_error:
		rlog_notice ('Failed to start array with command [%s]' % cmd_line)
		# md often leaves some badness around when it fails an assemble
		# remove it
		self.stop_raid_array(True)

	# since we failed assembly sometimes MD leaves some state around.
	# rescan our raid state.
	self.determine_array_status()

	return started_array
	
    #
    # try to start a raid array with uuid given -- returns true if started...
    #
    def __start_raid_array_with_uuid(self, uuid, dry = False):
	if self.__assemble_raid_array(uuid, dry):
	    for spare in self.__spare_device_list(uuid):
		dev_list = ['/dev/%s ' % spare.dev_name]
		opt_list = self.__form_mdadm_add_opt_list(spare)
		cmd_line = self.__form_mdadm_cmd_line(opt_list, dev_list)

		if dry:
		    print 'Running:', cmd_line
		else:
		    try:
		    	rlog_notice('Raid Add spares: [%s]' % cmd_line)
		    	run_shell_cmd(cmd_line)
		    except rrdm_error:
		    	rlog_notice('Failed to add spares with command [%s]' % cmd_line)

	    for invalid in self.__invalid_device_list(uuid):
		cmd_line = 'e2label /dev/%s ""' % invalid.dev_name

		if dry:
		    print 'Running:', cmd_line		
		else:
		    try:
		    	rlog_info('Raid clear invalid drive label: [%s]' % cmd_line)
		    	run_shell_cmd(cmd_line)
		    except rrdm_error:
		    	# we can ignore errors as it is very likely that the
		    	# invalid disk did not have a valid e2label
		    	pass

	    return True
	else:
	    return False
	
    def start_raid_array(self, dry = False):
        if self.status != 'stopped' and not dry:
	    # if its already running, just return ok
	    return

	plist = filter(lambda dev: not dev.hd.is_missing(), self.part_list)
	uuids = map(lambda dev: dev.raid_super().uuid(), plist)
	uuids = filter(lambda u: u != None and u != '', uuids)

	# remove duplicates in uuids...
	uuids.sort()
	nuuids = []
	prev = None
	for u in uuids:
	    if u != prev:
		nuuids.append(u)

	    prev = u

	uuids = nuuids

	# get our "expected uuid"
	uuid = SystemConfig().get_array_uid(self.name)
	array_started = False
	while len(uuids) > 0:
	    # first priority our uuid...
	    if uuids.count(uuid) > 0:
		u = uuid
	    else:
		# next priority, most uuids in list...
		maxu = max(map(lambda a: plist.count(a), uuids))
		u = filter(lambda a: plist.count(a) == maxu, uuids)[0]

	    uuids.remove(u)
	    if self.__start_raid_array_with_uuid(u, dry):
		array_started = True
		break

	if not array_started:
	    raise rrdm_error('failed to start RAID')
	else:
	    # raid array has started. If this raid array is a vecache then set the RAID 
	    # disable_queue_plugging sysfs param for this array
	    if self.__sysfscfg_list !=[]:
		# Setting sysfs param to disable queueing on RAID10 writes on the VE blockstore
		try:
		    for entry in self.__sysfscfg_list:
			cmd = 'echo %s > %s/%s/%s' % (entry.value, entry.type, self.dev_name, entry.path)
		    	run_shell_cmd(cmd)
		except IOerror, OSerror:
		    raise rrdm_error('Could not set sysfs param disable_queue_plugging for vecache device %s' % self.dev_name)
		     
 
    ###########################################################################
    # stop_raid_array
    #
    # if the raid array is already running, stop it. If ignore is specified
    # then run the stop command even if we think its not running, just to clean
    # up the leftover state.
    #
    ###########################################################################
    def stop_raid_array(self, force = False):
        if force or not self.is_stopped():
            cmd_line='mdadm --stop /dev/%s' % self.dev_name
	    try:
		run_shell_cmd(cmd_line)
	    except rrdm_error:
		raise rrdm_error('failed to stop RAID with cmdline : %s' % cmd_line)
        else:
            print 'Array %s is already stopped' % self.dev_name

###############################################################################
# Raid Devices
#
#
###############################################################################
class RaidDevices:
    def __init__(self):
        self.raid_arrays = []
        self.partition_map = []

    num_arrays = 0
    expected_num_arrays = 0
    raid_arrays = []
    sw_version	= "1.01.00"

    def get_num_arrays(self):
        return self.num_arrays

    def get_array_list(self):
        return self.raid_arrays

    def get_status(self, xml = False):
        if xml == True:
            print '<raid-devices num=\"%s\">' % self.num_arrays

        for array in self.raid_arrays:
            array.get_status(xml)

        if xml == True:
            print '</raid-devices>'

    def get_info(self):
	if self.num_arrays > 0:
	    print 'Software Raid Support \t\t:  True'
	    print 'Raid Software Version \t\t:  %s'   % self.sw_version
	    print 'Number of Raid Arrays \t\t:  %d'   % self.num_arrays 
	    print 'Expected of Raid Arrays \t:  %d' % self.expected_num_arrays 
	else:
	    print 'Software Raid Support \t:  False'

    def get_detail_status (self, xml = False):
        if xml == True:
            print '<raid-devices num=\"%s\">' % self.num_arrays

        for array in self.raid_arrays:
            array.get_detail_status(xml)

        if xml == True:
            print '</raid-devices>'

    def stop_raid_array(self):
	for array in self.raid_arrays:
	    array.stop_raid_array()

    def is_raid_array(self, name):
        for array in self.raid_arrays:
            if array.name == name:
                return 'true'
        return 'false'

    def check_config(self):
        if (self.num_arrays != self.expected_num_arrays):
            raise rrdm_error ('Raid Configuration Mismatch')

    def add_array (self, array):
        self.raid_arrays.append(array)
        self.num_arrays = self.num_arrays + 1

    def printto(self):
        print 'Num Raid Devices: %d' % self.num_arrays


