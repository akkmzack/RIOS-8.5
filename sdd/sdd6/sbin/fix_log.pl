#!/usr/bin/perl -w

#
# Copyright 2004 Riverbed Technology, Inc.
# All rights reserved. Confidential
#
# This script does post-processing on the list of
# operations generated by the Samba VFS logging shim.
# It provides input for the RCU unit, so we can determine
# what files have changed and need to be pushed to the 
# remote site
# read rules.txt for the magic rules used to condense
# the VFS logs... pass1 through pass4 make more sense then

# Useful stuff for debugging
#open STDOUT, '>', "/var/foo.out" or die "Can't redirect STDOUT: $!";
#open STDERR, ">&STDOUT"     or die "Can't dup STDOUT: $!";
#select STDERR; $| = 1;
#select STDOUT; $| = 1;

use Getopt::Std;
use Time::HiRes qw(gettimeofday);

sub numerically { $a <=> $b }

sub descending { $b <=> $a }

sub depth_first {
    $aa=$a;$bb=$b;
    ($bb =~ tr/\//\//) <=> ($aa=~ tr/\//\//) || lc($a) cmp lc($b);
}

#--------------------------------------------------------------
# Globals
#--------------------------------------------------------------

# path of the logging files
my $LOGPATH = "/var/log/rcu";

# the format of the log is: TIME|USER@IP|SHARE_NAME|ACTION|SUCCESS|FILE(or share if connect)|ACTION_SPECIFIC_STUFF
my $TIME_F = 0;
my $ACTION_F = 3;
my $SUCCESS_F = 4;
my $FILE_F = 5;
my $REN_F = 6;

# introduce time fudge factor to make the timestamps on rename_to ops very close to their parent renames,
# but not the same
# use BigFloat otherwise we get rounding issues...
use Math::BigFloat;
my $BLINK = Math::BigFloat->new(0.000001); 

# bug 9074 -- handle infinite looping in find_first_ren()
my $MAX_FIND_FIRST_REN = 10;
my $INVALID_RENAME_PATH = "__RBT_INVALID_RENAME_PATH__";

#--------------------------------------------------------------

#
# Handle any command-line arguments passed to the script.
#

sub get_cmd_args {

    # setup defaults...
    my($btime) = 0;
    my ($sec,$usec) = gettimeofday();
    my ($etime) = sprintf("%d.%.6d", $sec, $usec);
    my($log_path) = $LOGPATH;
    my($clean_old_log) = 0;
    my($share_path);
    my($share);

    # -b btime: first log time we're intesting in
    # -e etime: last log time we're interested in 
    # -o output file: where script output goes
    # -p log_path: where logging files are located.
    # -s share: which share's log files we are parsing
    # -l path to the share.
    getopts('hcb:e:o:p:s:l:');

    $btime = $opt_b if (defined $opt_b);
    $etime = $opt_e if (defined $opt_e);
    $log_path = $opt_p if (defined $opt_p);
    $clean_old_log = $opt_c if (defined $opt_c);
    $share = $opt_s if (defined $opt_s);
    $share_path = $opt_l if (defined $opt_l);

    if(defined $opt_o) {
	$output_path = $opt_o;
    } else {
	$output_path = "./";
    }

    if($opt_h || !(defined $opt_s) || !(defined $opt_l)) {
	print "fix_log.pl <options>. NOTE: -s and -l are required!\n";
 	print "-b <log begin time> -- 1st time we care about>\n";
	print "-c delete any old .rot files after script generates output\n";
	print "-e <log end time> -- Last time we care about>\n";
	print "-l <path to share> -- The top level dir for the share\n";
	print "-o <path for output files> -- where the output file for each share is placed\n";
	print "-p <path to log files> -- where log files are located\n";
	print "-s <share whose log files should be parsed>\n";
	$opt_h = 0; # prevent perl from complaining
	die;
    }

    return ($btime, $etime, $log_path, $clean_old_log, $share, $share_path, $output_path);
}

#--------------------------------------------------------------

# delete $log if last timestamp in the $log is less than $dtime
# else we need this file
sub delete_old_ts_log {
    my ($log, $dtime) = @_;

    # get last time in the log
    my $last_ts = `tail -1 $log | awk -F\\| '{ print \$1 }'`;
    chomp($last_ts);
    
    if(!($last_ts) || ($last_ts < $dtime)) {
	print "DELETING: file: $log last_ts: $last_ts dtime: $dtime\n";
	unlink($log);
    }
    else {
	print "KEEPING: file: $log last_ts: $last_ts dtime: $dtime\n";
    }
}

# 
# clean up any log files that have either been rotated, or
# are no longer in use, and have times that are older than
# the specified time
#

sub delete_logs {

    my ($dtime, $log_path, $share) = @_;

    print "Cleaning old logs\n";

    # form a list of all the logs that could be rotated.
    # bug 7740 -- we now grab all files present (rbt_audit.*), instead of
    # just .rot files -- that's because we'll do a check to figure out which
    # logs are in use down below.

    # bug 3417 .. one dir per share so that's why the $share in path below 
    # need to use readdir since ls or find in a shell context can break if there are too many files
    my $dir = "$log_path/$share";
    opendir(DIR, $dir) or die "Couldn't open $dir";
    my @logs = grep {/rbt_audit\.$share\./} readdir(DIR);
    close(DIR);
    
    my $backup_dir = "$log_path/backup/$share";

    `mkdir -p $backup_dir`;

    # for running processes keep pid and file timestamps to delete all logs
    # except for the newest one
    my %running_pids = ();
    
    foreach my $tmp_log (@logs) {

	$log = "$log_path/$share/$tmp_log";

	# for each log, figure out the pid, and make sure that there is
	# nothing else running with that pid.
	my @entries = split(/\./, $log);
	my $pid = $entries[2]; # always the 3rd entry


	# skip this log if samba is running w/ that pid -- we'll get it
	# next time around
	my $running = `ps -p $pid | grep smbd | wc | awk '{ print \$2 }'`;
	chomp($running);
	print "running = $running\n";

	if($running) {
	    print "FILE: $log is from a running PID: $pid\n";
	    push @{$running_pids{$pid}}, $entries[3]; # the ts
	    next;
	}
	    
	delete_old_ts_log($log, $dtime);
    }

    # process logs from running pids
    foreach my $pid (keys %running_pids) {

	@times = sort numerically @{$running_pids{$pid}};
	$size = scalar @times;
	print "Checking running process, PID: $pid SIZE: $size\n";

	# don't handle single-log pids since we may still write to that file
	if( $size <= 1) {
	    print "Skipping singleton pid: $pid\n";
	    next;
	}

	for(my $i=0; $i < $size-1; $i++) {
	    my $ts = $running_pids{$pid}[$i];
	    my $file = "$dir/rbt_audit.$share.$pid.$ts"; # full path to logfile
	    delete_old_ts_log($file, $dtime);
	}
    }
}


#--------------------------------------------------------------

#
# find the 1st directory in a rename list chain
#
sub find_first_ren {

    # name is the final name in the chain
    my ($ren_to, $ren_time, $ren_from) = @_;

    # bug 9074 -- this is to avoid directories that were renamed to each other
    # multiple times between runs of this script. Current heuristic is that if
    # the rename chain is too long, we will NOT try to do a rename, and
    # instead handle this rename in the way we do files.
    my $count = 0;
    
    my $fst_name = $ren_to;
    my $tmp_name = $ren_to;
    
    TOP: while(defined($ren_from->{$tmp_name})) {

	if($count > $MAX_FIND_FIRST_REN) {
	    return $INVALID_RENAME_PATH;
	}
	$count++;
	
	# get the last entry in the list that occurred before the specified time
	FOR: foreach my $time (sort descending keys %{$ren_from->{$tmp_name}}) {
	    if ($time < $ren_time) {
		
		# if the name is New Folder, then we're done...
		my $path = $ren_from->{$tmp_name}{$time};
		if($path =~ /^New Folder$/) {
		    last TOP;
		}else {
		    $tmp_name = $ren_from->{$tmp_name}{$time};
		    next TOP;
		}
	    }
	}
    }

    return $tmp_name;
}






#--------------------------------------------------------------
# debugging funcs
#--------------------------------------------------------------
sub print_file_actions {

    my ($files, $share, $outfh) = @_;

    print $outfh "*********************************************\n";
    print $outfh "Share: $share\n";

    foreach my $file (sort keys %$files) {
	print $outfh "----------------------------------------\n";
	print $outfh "File: $file\n";

	my @operations = @{$files->{$file}};
	
	for( my $i =0; $i < @operations; $i++) {
	    my $op = $operations[$i];
	    print $outfh "time: $op->{time} action: $op->{action} params: @{$op->{params}}\n";
	}
    }
}

sub dump_complete_log {
    die "this function no longer works!!!";

    # XXX/PM this no longer works... not used and I'm not converting it to work w/ a dir per share 
    my ($entries, $share) = @_;
    open(COMPLETE_LOG, ">$LOGPATH/rbt_audit.$share.complete") or die "Couldn't open $LOGPATH/rbt_audit.$share : $!";

    foreach my $entry(sort numerically keys %{$entries->{$share}}) {
	print COMPLETE_LOG "$entries->{$share}{$entry}\n";
    }
    close(COMPLETE_LOG);
}

#--------------------------------------------------------------


# pass 2
# at this point $files{$filename} contains all ops for a given file for this share
# we purge actions that are cancelled by other actions, see rules.txt for more info
sub pass_2 {
    my ($files) = @_;

    foreach my $file (sort keys %$files) {

	my @operations = @{$files->{$file}};
	my @kept_ops = ();
	my $should_keep = 1;	# whether or not we keep this operation during purge step.
	for( my $i = $#operations; $i >= 0; $i--) {

	    my $act = $operations[$i]->{action};

	    if($act =~ /rename_to|mkdir|open/) {
		$should_keep = 1;
		unshift @kept_ops, $operations[$i];
	    }
	    elsif($act =~ /rename$|rmdir|unlink/) {
		$should_keep = 0;
		unshift @kept_ops, $operations[$i];
	    }
	    else {
		unshift(@kept_ops, $operations[$i]) if($should_keep);
	    }
	}
	$files->{$file} = [@kept_ops]; # store modifications
    }
}


#--------------------------------------------------------------

# pass 3 -- purge subsequent cancelling operations. Only
# do this if they immediately follow each other
sub pass_3 {

    my ($files) = @_;

    foreach my $file (sort keys %$files) {
	
	my @operations = @{$files->{$file}};
	my $size = scalar @operations;
	my @kept_ops;

	# skip any ops which don't have atleast 2 operations
	next unless ($size > 1);

	for( my $i = 0; $i < $size;) {

	    # if we've hit the last entry, just store it away, and we're done...
	    if($i == ($size -1)) {
		push @kept_ops, $operations[$i];
		last;
	    }

	    my $act = $operations[$i]->{action};
	    my $next_act = $operations[$i+1]->{action};

	    # case1: if mkdir then rename or rmdir
	    if(($act =~ /mkdir/) and ($next_act =~ /rename$|rmdir/)) {
		# don't store... but skip to next set of actions
		$i += 2;
		
	    }
	    # if rename_to_me then unlink or rename
	    elsif( ($act =~ /rename_to/) and ($next_act =~ /unlink|rename$/)) {
		# don't store...
		$i += 2;
	    }
	    else {
		push @kept_ops, $operations[$i];
		$i++;
	    }
	}
	$files->{$file} = [@kept_ops]; # store modifications
    }
}


#-----------------------------------------------------------------------------

# pass 4 - print out a list of any files that changed, and those that were deleted.
# changed implies either previously existed, or newly created.
# also, note that we may say certain files were deleted that don't exist, because
# they were created during the run and then unlinked. This is not a bug!
sub pass_4 {

    my ($files, $outfh, $ren_from, $share_path) = @_;

    my %final_output = ();

    # pass 4 -- final phase -- end output should be which files changed or were deleted.
    FILE_TO_PROCESS: foreach my $file (sort depth_first keys %$files) {
	my @operations = @{$files->{$file}};
	my $size = scalar @operations;

	# skip any files w/ no entries...
	next unless ($size);

	# any file where the last action is a rename, rmdir, or unlink is reported as deleted...
	my $last_act = $operations[$#operations]->{action};
	my $last_time = $operations[$#operations]->{time};

	if($last_act =~ /rename$|rmdir|unlink/) {
	    if (!defined($final_output{$file})) {
		$final_output{$file} = "DELETE: $file";
	    }
	}

	# otherwise the file has changed...
	else {
	    
	    # if it's a rename and we know that the file is a dir, then we find 1st dir in the rename chain
	    my $full_path = "$share_path/$file";

	    my $orig_file = $file; # since find_first_ren may modify file...
	    
	    if( ($last_act =~/rename_to/) && (-d $full_path) ) {
		my $first_file = find_first_ren($file, $last_time, $ren_from);

		# bug 9074 -- error case, we if we get back INVALID_RENAME_PATH then it's likely we would've
		# ended up in an infinite loop, so instead note that the file has "CHANGED".
		if($first_file eq $INVALID_RENAME_PATH) {
		    $final_output{$orig_file} = "CHANGED_DATA: $file";
		    next FILE_TO_PROCESS;
		}

		# bug 3723 -- we should also handle renames from New Folder to something else...
		# in this case, find_first_ren returns the file you input
		if($first_file eq $file) {
		    $final_output{$file} = "RENAME_OLD: New Folder\nRENAME_NEW: $file";
		    $final_output{"New Folder"} = "";
		} else {
		
		    # rename output should be on 2 separate lines, with old one first, then new
		    $final_output{$file} = "RENAME_OLD: $first_file\nRENAME_NEW: $file";

		    # want to remove any potential DELETE for first_file (also prevents first_file insertion!)
		    $final_output{$first_file} = "";
		}
	    } else {

		# go through all the operations to see if the file has a (p)write/truncate/open/fsync/rename_to
		# in this case the data has changed, otherwise it's just the attributes.
		# NOTE: open in this case already implies open w/ create/write flags set.
		my $data_modified = 0;
		for (my $i =0; $i <= $#operations; $i++) {
		    my $act = $operations[$i]->{action};
		    if ($act =~ /write|truncate|fsync|open|rename_to/) {
			$data_modified = 1;
		    }
		}
		if($data_modified == 1) {
		    $final_output{$file} = "CHANGED_DATA: $file";
		} else {
		    $final_output{$file} = "CHANGED_ATTR: $file";
		}
	    }
	}
    }

    foreach my $file (sort depth_first keys %final_output) {
	next if($final_output{$file} eq "");
	print $outfh "$final_output{$file}\n";
    }
}


#--------------------------------------------------------------
# main
#--------------------------------------------------------------

my($btime); # starting time we care about in logs
my($etime); # ending time we care about in logs
my($log_path); # path to log files
my($clean_old_log); # should we delete old .rot files?
my($share); # share whose log files we're parsing
my($share_path); # path to the top level dir of the share
my($output_path); # path to the location where we dump the .out files

($btime, $etime, $log_path, $clean_old_log, $share, $share_path, $output_path) = get_cmd_args();

# currently we only do 1 share at a time, the one specified in -s
my @shares = ($share);

# All log entries... indexed by a given share.
my %entries = ();

# All the renames that occur... it is indexed by the new name, and has info on the old name
# XXX/PM: This will fail horribly if we need to do multiple share parsing again, it will need to be fixed
# to be present on a per-share basis!
my %ren_from = ();

# this will fail if we ever switch to multiple shares...
my $sh_size =  scalar @shares;
die "# of shares must be 1!" if ($sh_size != 1);

# for each share, we want a single list containing all logging for that share
foreach my $share (@shares) {

    # clean up any logs with a time less than the begin time, so we have less clutter...
    if($btime) {
	delete_logs($btime, $log_path, $share);
    }

    # need to use readdir since ls or find in a shell context can break if there are too many files
    my $dir = "$log_path/$share";
    opendir(DIR, $dir) or die "Couldn't open $dir";
    my @files = grep {/rbt_audit\.$share\./} readdir(DIR);
    close(DIR);


    foreach my $tmp_file (@files) {
	my $file = "$log_path/$share/$tmp_file";
	open(LOG, "$file") or die "Couldn't open $file : $!";

	# see if the last ts in the file is already before the btime, if
	# so, then skip it...
	my $last_ts = `tail -1 $file | awk -F\\| '{ print \$1 }'`;
	chomp($last_ts);
	if($last_ts && $last_ts < $btime) {
	    print "skipping file: $file last_ts: $last_ts\n";
	    close (LOG);
	    next;
	}

	# pass 1
	while(<LOG>) {
	    chomp;
	    my @entry = split(/\|/);

	    # get rid of the ./ that may be present at the beginning of a file.
	    $entry[$FILE_F] =~ s/^\.\///;

	    next if( ($entry[$TIME_F] < $btime) || ($entry[$TIME_F] > $etime) );


	    $entries{$share}{$entry[$TIME_F]} = $_;

	    # for renames we want to store a "rename_to" for the file we're renaming to, makes life easier later on.
	    if($entry[$ACTION_F] =~ /^rename$/) {

		# get rid of the ./ that may be present at the beginning of a file.
		$entry[$REN_F] =~ s/^\.\///;

		my @twin = @entry;
		$twin[$ACTION_F] = "rename_to";
		$twin[$TIME_F] += $BLINK;
		my $source_file = $twin[$FILE_F];
		$twin[$FILE_F] = $twin[$#twin]; # this is the destination file
		$twin[$#twin] = $source_file;
		$entries{$share}{$twin[$TIME_F]} = join("|", @twin);

		my $from = $entry[$FILE_F];
		my $time = $entry[$TIME_F];
		my $to = $entry[$REN_F];
		$ren_from{$to}{$time} = $from;

	    }
	}
	close(LOG);
    }
}

# At this point, the complete log for each share is in $entries{$share}
foreach my $share (@shares) { 

    # the output file for the share is share.out
    my $outfh;
    open($outfh, ">>$output_path/$share.out") or die "Can't open $share.out for writing: $!";

    # the actions on a given file for a share...
    my %files = ();

    foreach my $entry(sort numerically keys %{$entries{$share}}) {
	my @fields = split(/\|/, $entries{$share}{$entry});

	# skip any failed actions
	next if ($fields[$SUCCESS_F] =~ m/fail/);
    
	# skip connect and disconnect actions
	next if ($fields[$ACTION_F] =~ m/connect/);

	# get rid of the ./ that may be present at the beginning of a file.
	$fields[$FILE_F] =~ s/^\.\///;

	# assert if we have out of bounds time...
	if( ($fields[$TIME_F] < $btime) || ($fields[$TIME_F] > $etime) ) {
	    die "out of bounds time: $fields[$TIME_F] btime: $btime etime: $etime" ;
	}
	
	
	# for each action, store away action and associated params w/ the file
	my $file_name = $fields[$FILE_F];

	my $action = $fields[$ACTION_F];
	my @params = @fields[$FILE_F+1..$#fields];
	my $operation = {
	    action => $action,
	    params => [@params],
	    time => $fields[$TIME_F],
	};

	# this operation is just one of many for this file...
	push @{$files{$file_name}}, $operation;
    }

    # pass_2 see func for more details
    pass_2(\%files);

    # pass_3 -- see func for more details
    pass_3(\%files);

    print $outfh "FINAL_OUTPUT. Share: $share btime: $btime etime: $etime\n";

    # pass 4 - see func for more details
    pass_4(\%files, $outfh, \%ren_from, $share_path);

    close($outfh);
}

#-----------------------------------------------------------------------------

# if cleanup is specified, then delete old rotated logs...
if($clean_old_log) {
    delete_logs($etime, $log_path, $share);
}
