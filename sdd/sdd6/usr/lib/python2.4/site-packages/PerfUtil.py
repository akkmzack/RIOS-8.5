from sys import exit
from os import popen
from os.path import exists
from re import compile as recompile


## PerfTestType
# class describing types of tests, be it read/write/etc
# any test with a write component is labelled destructive
# and the caller can decide how to handle destructive tests
#
# For example, if we do a destructive test for the segstore,
# we want to leave the segstore header clean, and signal sport
# to start clean.
#
class PerfTestType:
    test_type_read      = 'read'
    test_type_write     = 'write'
    test_type_readwrite = 'readwrite'
    test_type_list = [ test_type_read,
                       test_type_write,
                       test_type_readwrite ]

    ## is_valid_type
    # @param type string representation of the type
    #
    # returns True if the type is valid
    #         False otherwise
    def is_valid_type(self, type):
        return type in PerfTestType.test_type_list

    ## is_destructive
    # @param type type string representation assumed to be checked as valid
    # 
    # returns True if the type has a write component
    #         False otherwise
    def is_destructive(self, type):
        return type in [ self.test_type_write, 
                         self.test_type_readwrite ]

## PerfTestUtility
# class wrapping a performance utility config node
#
# The class corresponds to the utility xml node
#     <utility name="diskperf"
#             arguments="-r -a random -t #time# -d #device# -s #size#"
#             type="read"/>
#
# The utility arguments variable accepts 3 special #var# tags that can
# be substituted for user specified variables
# 1) #time# duration of the test
# 2) #size# disk size for use with the test
# 3) #device# target device for use with the test
#
# get_cmdline_raw returns the actual config string without substitutions
# get_cmdline returns the actual command line the above user 
#             arguments translated
#
class PerfTestUtility:
    ## Initializer
    # @param xml utility xml node
    #
    def __init__(self, xml):
        self._name = ''
        self._arguments = ''
        self._argument_list = []
        self._type = ''

        if xml == None:
            raise AssertionError('Invalid xml argument')
   
        self.__populate_from_xml(xml)
   
    ## __str__
    # return a textual representation of the object
    def __str__(self):
        result = ''
        result += '%s/name=%s\n' % (self._name, self._name)
        result += '%s/cmdline=%s %s\n' % (self._name,
                                             self._name,
                                             self._arguments)
        return result

    ## __populate_from_xml
    # @param xml the utility xml node
    # Reads in class members from the xml and does validity checking
    def __populate_from_xml(self, xml):
        self._name      = xml.getAttribute('name')
        self._arguments = xml.getAttribute('arguments')
        self._argument_list = self._arguments.split(' ')
        type            = xml.getAttribute('type')

        if not PerfTestType().is_valid_type(type):
            raise AssertionError('Invalid type %s in utility definition' % type)
        else:
            self._type = type

    ## get_name
    # Returns the utility name
    def get_name(self):
        return self._name

    ## is_destructive
    # Indicates whether this test has written data to the system
    # that requires cleanup after the test (e.g. a write disk test)
    #
    def is_destructive(self):
        if self._type != '':
            if PerfTestType().is_valid_type(self._type):
                return PerfTestType().is_destructive(self._type)

        raise AssertionError('Invalid utility test type %s' % self._type)
                

    ## get_cmdline_raw
    # Returns the command line directly from the config without any
    # translations of the 
    def get_cmdline_raw(self):
        return '%s %s' % (self._name, self._arguments)

    ## convert_arg
    # @param arg config string for utility arguments
    # @param time user specified time
    # @param device user specified device
    # @param size user specified size
    # 
    # This routine converts any special tags in the argument string
    # to values provided by test application
    def __convert_arg(self, arg, time, device, size):
        dev_arg = '#device#'
        time_arg = '#time#'
        size_arg = '#size#'
        if arg in [ dev_arg, time_arg, size_arg ]:
            if arg == dev_arg:
                if device == None:
                    raise AssertionError('No device argument specified')
                else:
                    return device
            elif arg == time_arg:
                if time == None:
                    raise AssertionError('No time argument specified')
                else:
                    return time
            elif arg == size_arg:
                if size == None:
                    raise AssertionError('No size argument specified')
                else:
                    return size
        else:
            return arg

    ## get_cmdline
    # @param time test duration
    # @param device optional argument for target device
    # @param size optional argument for test size (such as disk size)
    #
    # Returns the translated command line associated with this utility
    #
    def get_cmdline(self, time, device=None, size=None):
        arg_result = map(lambda arg: 
                         self.__convert_arg(arg, time, device, size), 
                         self._argument_list)
        return '%s %s' % (self._name, ' '.join(arg_result))

    ## get_test_result
    # @param output test output
    # This returns a TestResult object based on the type of utility used
    def get_test_result(self, output):
        if self._name == 'diskperf':
            return DiskPerfTestResult(output)
        elif self._name == 'cputest':
            return CpuTestResult(output)
        else:
            raise AssertionError('Invalid utility')

#### Test Result Objects
## TestResult
# Generic test result super class
# Objects of this type are not meant to be instantiated directly
# but instead are expected to be sub-classed for particular utility
# parsing
class TestResult:
    ## Initializer
    # @param output test output buffer
    def __init__(self, output):
        self._result_map = {}

        self._output = output

    ## __str__
    # returns a textual representation of the TestResult object
    def __str__(self):
        result = ''
        for key in self._result_map.keys():
            result += '%s=%s\n' % (key, self._result_map[key])

        return result

    ## __parse_output
    # virtual function
    def __parse_output(self):
        raise AssertionError('Do not run this')

    ## get_keys
    # Returns a list of keys associated with the TestResult object
    def get_keys(self):
        return self._result_map.keys()

    ## get_value
    # @param key key to use for indexing the TestResult dictionary
    # returns the value associated with a particular key
    def get_value(self, key):
        if self._result_map.has_key(key):
            return self._result_map[key]
        else:
            return None

#### DiskPerf Related Objects

## DiskPerfTestResult
# DiskPerf result object, specifically for tests utilizing the
# diskperf tool
class DiskPerfTestResult(TestResult):
    # Class specific local variables for parsing diskperf output
    avg_read_rate_key  = '^Avg read rate'
    avg_write_rate_key = '^Avg write rate'
    # the keys here need to be synchronized with the test_result
    # xml metrics of the same name
    avg_read_map_key   = 'read_tput'
    avg_write_map_key  = 'write_tput'
    diskperf_value_list = [ avg_read_rate_key,
                            avg_write_rate_key ]

    ## Initializer
    # @param output test output
    def __init__(self, output):
        TestResult.__init__(self, output)
        self.__parse_output()

    ## __extract_rate
    # @param line a given line that has matched our extractor regex
    # Returns a specific value for a given DiskPerf output line
    # we are interested in
    #
    def __extract_rate(self, line):
        parts = line.split('=')
        if len(parts) >= 2:
            return parts[1].strip().split(' ')[0]
        else:
            return None

    ## __parse_output
    # take the output blob and split it into lines, which we'll 
    # check against particular disk perf regex values for each matching line
    # we append an entry to the result map indexed by the particular key
    # The key used in the result map should correspond to a metric key
    # identified in the test model specific results
    #
    def __parse_output(self):
        rr_regex = recompile(self.avg_read_rate_key)
        wr_regex = recompile(self.avg_write_rate_key)

        lines = self._output.split('\n')
        for line in lines:
            if rr_regex.match(line):
                self._result_map[self.avg_read_map_key] = \
                    self.__extract_rate(line)
            if wr_regex.match(line):
                self._result_map[self.avg_write_map_key] = \
                    self.__extract_rate(line)

## CpuTestResult class
# This class is currently not used, but is an example how we would
# implement a cpu performance test that returns an amount of time 
# needed to complete a particular set of operations 
#
class CpuTestResult(TestResult):
    ## test_run_time_key
    # This assumes this CPU test will return the amount of time
    # needed to run a particular test
    test_run_time_key = 'run_time'
    cpu_test_value_list = [ test_run_time_key ]

    ## Initializer
    # @param output test result output
    def __init__(self, output):
        TestResult.__init__(self, output)
        self.__parse_output()

    ## __extract_rate
    # @param line the line from the cpu test output that contains the 
    #        test result
    def __extract_rate(self, line):
        parts = line.split('=')
        if len(parts) >= 2:
            return parts[1].strip().split(' ')[0]
        else:
            return None

    ## __parse_output
    # cputest specific output parser
    def __parse_output(self):
        rt_regex = recompile(self.test_run_time_key)

        lines = self._output.split('\n')
        for line in lines:
            if rt_regex.match(line):
                self._result_map[self.test_run_time_key] = \
                    self.__extract_rate(line)
