from xml.dom.minidom import parse
from sys import exit
from os import popen
from os.path import exists
from re import compile as recompile

## ModelPerfMetrics
# Each model test shall have a metric associated with it. Each metric shall 
# have an expected value, a min value and a max value associated
# with it.  Also a test may have multiple metrics associated with it
# i.e. for a disk test with read/write components, there will be
# 2 metrics one read one write.
# 
# a metric corresponds to a result entry:
#        <metric name="read_tput" value="5000" min_value="4000" max_value="0"/>
#
class ModelPerfMetrics:
    expected_val_key = 'value'
    min_val_key      = 'min_value'
    max_val_key      = 'max_value'
    
    valid_keys = [ expected_val_key,
                   min_val_key,
                   max_val_key ]

    ## Initializer
    # @param xml xml metric node
    #
    def __init__(self, xml):
        self._metric_map = {}

        if xml == None:
            raise AssertionError('Invalid XML passed for ModelPerfMetric()')

        self._metric_map[self.expected_val_key] = xml.getAttribute(self.expected_val_key)
        self._metric_map[self.min_val_key]      = xml.getAttribute(self.min_val_key)
        self._metric_map[self.max_val_key]      = xml.getAttribute(self.max_val_key)

    ## get_value
    # @param key key for looking up a particular metric value
    def get_value(self, key):
        if self._metric_map.has_key(key):
            return self._metric_map[key]
        else:
            return None

    ## compare_value
    # @param value string integer value for comparison with this metric
    #
    # Function for use when comparing a measured value against a particular spec
    # Normally if simply compares if int(value) is between min/max
    # if min is 0, we assume that the value simply must be <= max
    # if max is 0, we assume that the value must be >= min
    #
    def compare_value(self, value):
        min_val = int(self.get_value(self.min_val_key))
        max_val = int(self.get_value(self.max_val_key))
        int_val = int(value)

        if min_val == 0 and max_val > 0:
            if int_val <= max_val:
                return True
        elif min_val > 0 and max_val == 0:
            if int_val >= min_val:
                return True
        elif min_val > 0 and max_val > 0:
            if max_val < min_val:
                raise AssertionError('Invalid metric xml with max_val %d < min_val %d' % (
                                     max_val, min_val))
            else:
                if int_val >= min_val and int_val <= max_val:
                    return True

        return False

## Model Performance Result 
# Each model can be associated with a number of test results 
# for a given test. The methodology for model perfomance results is
# to create a dictionary of performance metrics indexed by 
# the metric name.
#
# The Utility parsers and the Model Perf Result xml spec need
# to agree on a name for the metrics.  If they do, then it is simple 
# to look up the min/max/required value for a given perf 
# test/measurement.
#
# Corresponds to a model node such as :
#
#    <model name="V250L">
#        <metric name="read_tput" value="5000" min_value="4000" max_value="0"/>
#    </model>
#
class ModelPerfResult:
    ## Initializer
    # @param xml xml node for a test model
    def __init__(self, xml):
        self._name = ''
        # used to be value map
        self._model_metric_map = {}

        self.__populate_from_xml(xml)

    ## __str__
    # returns a string representation of the object
    def __str__(self):
        result = ''
        
        for key in self._value_map.keys():
            result += '%s/%s=%s\n' % (self._name,
                                      key,
                                      self._value_map[key])
        return result

    ## __populate_from_xml
    # @param xml model xml node 
    # reads the values from XML and populates the object
    #
    def __populate_from_xml(self, xml):
        self._name = xml.getAttribute('name')

        metrics = xml.getElementsByTagName('metric')
        for metric in metrics:
            name         = metric.getAttribute('name')
            model_metric = ModelPerfMetrics(metric)
            self._model_metric_map[name] = model_metric

    ## get_name
    # Returns the model name associated with the perf result
    def get_name(self):
        return self._name

    ## get_results
    # Returns the dictionary associated with this 
    def get_results(self):
        return self._value_map

    ## get_keys
    # return a list of metric name keys associated with this model perf result
    def get_keys(self):
        return self._model_metric_map.keys()

    ## get_metric 
    # @param key the name of a metric key associated with this object
    # returns a metric object associated with this models results
    def get_metric(self, key):
        if self._model_metric_map.has_key(key):
            return self._model_metric_map[key]
        else:
            return None

    ## get_metric_value
    # @param metric_key name of a metric key
    # @param value_key name of a metric attribute
    # Returns the value of a metric sub-field
    def get_metric_value(self, metric_key, value_key):
        if self._model_metric_map.has_key(metric_key):
            return self._model_metric_map[metric_key].get_value(value_key)
        else:
            return None

## Model Perf Result Map
# Dictionary of model performance results for a given test, indexed by model
# name
class ModelPerfResultMap:
    ## Initializer
    # @param xml node containing the model results for a given test
    # 
    def __init__(self, xml):
        self._model_results = {}
        if xml == None:
            raise AssertionError('Invalid xml for rvbd test result')
    
        for model_result in xml.getElementsByTagName('model'):
            rmr = ModelPerfResult(model_result)
            self._model_results[rmr.get_name()] = rmr

    ## __str__ 
    # Return a string object representation
    def __str__(self):
        result = ''
        for model_val in self._model_results.values():
            result += '%s' % str(model_val)
        return result

    ## get_results
    # Return a list of ModelPerfResults associated with this map
    def get_results(self):
        return self._model_results.values()

    ## get_results_by_model
    # @param model
    # Return the model results for a given model
    def get_result_by_model(self, model):
        if self._model_results.has_key(model):
            return self._model_results[model]
        else:
            return None

## Performance test result map
# Associates each performance test with a set of results associated with
# each supported model
class PerfTestResultMap:
    def __init__(self, fname):
        self._fname = fname
        self._test_result_map = {}

        if not exists(fname):
            raise AssertionError('%s file does not exist' % fname)

        xml = parse(fname)

        test_results = xml.getElementsByTagName('test_result')
        for result in test_results:
            name = result.getAttribute('name')
            mod_res_map = ModelPerfResultMap(result)

            self._test_result_map[name] = mod_res_map

    ## __str__ 
    # return a string representation of the object
    def __str__(self):
        result = ''
        for test in self._test_result_map.keys():
            mres = self._test_result_map[test]

            for res in mres.get_results():
                result += '%s/%s' % (test, res)

        return result

    ## get_model_results
    # @param name Name of a particular performance test
    # given a test name, get the associated model results
    def get_model_results(self, name):
        if self._test_result_map.has_key(name):
            return self._test_result_map[name]
        else:
            return None

    ## get_model_results_by_test_model
    # @param name Name of a particular performance test
    # @param model particular model we want results for
    # given a test name and model get the specific model results
    def get_model_results_by_test_model(self, name, model):
        if self._test_result_map.has_key(name):
            return self._test_result_map[name].get_result_by_model(model)
        else:
            return None
